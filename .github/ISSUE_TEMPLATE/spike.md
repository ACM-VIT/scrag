---
name: Discovery/Spike Task
about: Create a discovery or spike task for research and exploration
title: '[SPIKE] '
labels: 'spike, research'
assignees: ''
---

## Discovery Objective

**Spike Title:** [Brief descriptive title]
**Parent Epic:** [Link to parent epic - #epic-number]
**Time Box:** [X days/weeks - recommend 1-2 weeks max]
**Priority:** [High/Medium/Low]

## Problem Statement

<!-- What specific question or problem does this spike address? -->

## Research Questions

<!-- List specific questions this spike should answer -->

- [ ] [Question 1 - e.g., "What is the performance difference between X and Y?"]
- [ ] [Question 2 - e.g., "How does integration with Z work?"]
- [ ] [Question 3 - e.g., "What are the security implications of approach A?"]

## Success Criteria

<!-- Define what constitutes a successful spike -->

- [ ] [Specific deliverable 1]
- [ ] [Specific deliverable 2]
- [ ] [Decision/recommendation ready]

## Research Approach

### Investigation Methods
- [ ] [Literature review / documentation research]
- [ ] [Proof of concept implementation]
- [ ] [Performance benchmarking]
- [ ] [Expert consultation]
- [ ] [Competitor analysis]

### Evaluation Criteria
- [ ] [Criteria 1 - e.g., "Performance requirements"]
- [ ] [Criteria 2 - e.g., "Integration complexity"]
- [ ] [Criteria 3 - e.g., "Maintenance overhead"]

## Constraints

- **Time Limit:** [X days] - Hard stop regardless of completion
- **Scope Limit:** [What's explicitly out of scope]
- **Resource Limit:** [Any resource constraints]

## Expected Deliverables

### Research Documentation
- [ ] Spike findings document in `/research/spikes/[spike-name]/`
- [ ] Executive summary with clear recommendations
- [ ] Technical deep-dive with implementation details
- [ ] Risk assessment and trade-off analysis

### Code Artifacts (if applicable)
- [ ] Proof-of-concept implementation
- [ ] Benchmark scripts and results
- [ ] Configuration examples

### Decision Support
- [ ] Architecture Decision Record (ADR) if architectural choice
- [ ] Implementation plan for recommended approach
- [ ] Effort estimation for implementation

## Research Areas

<!-- Mark applicable areas -->

- [ ] Performance evaluation
- [ ] Security analysis
- [ ] Integration patterns
- [ ] Third-party library evaluation
- [ ] Architecture design
- [ ] User experience research
- [ ] Infrastructure requirements
- [ ] Other: [specify]

## References

<!-- Link to relevant documentation, similar solutions, etc. -->

- [Reference 1 - existing documentation]
- [Reference 2 - relevant issue/PR]
- [Reference 3 - external resource]

## Definition of Done

A spike is considered complete when:

- [ ] All research questions have been addressed or explicitly marked as out-of-scope
- [ ] Findings document is complete and reviewed
- [ ] Clear recommendation is provided with rationale
- [ ] Executive summary suitable for stakeholder communication
- [ ] All code artifacts are committed to appropriate location
- [ ] Follow-up implementation tasks are identified and estimated
- [ ] Risks and trade-offs are clearly documented
- [ ] Knowledge is captured in `/research/` directory for future reference

## Success Metrics

<!-- Define how to measure spike success -->

- [ ] [Metric 1 - e.g., "Decision confidence level: High/Medium/Low"]
- [ ] [Metric 2 - e.g., "Implementation effort estimated within Â±25%"]
- [ ] [Metric 3 - e.g., "All stakeholders agree with recommendation"]

## Follow-up Actions

<!-- To be filled during/after spike completion -->

- [ ] [Implementation task 1 to be created]
- [ ] [Implementation task 2 to be created]
- [ ] [Documentation update needed]
- [ ] [Additional research required]

## Notes

<!-- Additional context, assumptions, constraints -->